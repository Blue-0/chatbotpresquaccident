'use client';
import React, { useState, useRef, useEffect } from "react";
import { useSession, signOut } from 'next-auth/react'
import { useRouter } from 'next/navigation'
import Particles from "@/src/components/Particles";
import {
    Card,
    CardContent,
    CardDescription,
    CardFooter,
    CardHeader,
    CardTitle
} from "@/src/components/ui/card";
import {Button} from "@/src/components/ui/button";
import {SessionId} from "@/app/components/sessionid";
import { useSessionId } from "@/app/hooks/useSessionId";
import {marked} from "marked";

interface Message {
    id: string;
    type: 'user' | 'ai';
    content: string;
    timestamp: Date;
}

export default function ChatPage() {
    const { data: session, status } = useSession()
    const router = useRouter()
    const { sessionId } = useSessionId();
    const messagesEndRef = useRef<HTMLDivElement>(null);
    const textareaRef = useRef<HTMLTextAreaElement>(null);
    const [messages, setMessages] = useState<Message[]>([
        {
            id: '1',
            type: 'ai',
            content: 'Bonjour ! Je suis votre assistant E2I AgentSecu. Comment puis-je vous aider aujourd\'hui ?',
            timestamp: new Date()
        }
    ]);
    const [inputMessage, setInputMessage] = useState('');
    const [isRecording, setIsRecording] = useState(false);
    const [isTranscribing, setIsTranscribing] = useState(false);
    const [isSpeaking, setIsSpeaking] = useState<string | null>(null);
    const mediaRecorderRef = useRef<MediaRecorder | null>(null);
    const audioChunksRef = useRef<Blob[]>([]);
    const transcriptionBufferRef = useRef<string>('');
    const pendingTranscriptionsRef = useRef<Set<number>>(new Set());
    const recognitionRef = useRef<any>(null);
    const [isLiveTranscribing, setIsLiveTranscribing] = useState(false);

    // Rediriger vers login si pas de session
    useEffect(() => {
        if (status === 'loading') return // Encore en chargement

        if (!session) {
            router.push('/Login')
            return
        }
    }, [session, status, router])

    // Auto-scroll vers le bas √† chaque nouveau message
    useEffect(() => {
        messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
    }, [messages]);

    // Auto-resize du textarea
    useEffect(() => {
        if (textareaRef.current) {
            textareaRef.current.style.height = '44px'; // Reset √† la hauteur minimale
            const scrollHeight = textareaRef.current.scrollHeight;
            const maxHeight = 120; // Hauteur maximale (environ 4 lignes)
            textareaRef.current.style.height = Math.min(scrollHeight, maxHeight) + 'px';
        }
    }, [inputMessage]);

    // Fonction optimis√©e pour d√©marrer l'enregistrement Voxtral selon les bonnes pratiques Mistral AI
    const startVoxtralLiveTranscription = async () => {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    channelCount: 1, // Mono recommand√© par Mistral
                    sampleRate: 16000, // 16kHz optimal pour Voxtral
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true // Ajout pour stabiliser le volume
                }
            });
            
            // Configuration optimis√©e selon la doc Mistral AI
            const supportedMimeTypes = [
                'audio/wav',
                'audio/mp4', // M4A support√©
                'audio/webm;codecs=opus', // Fallback √† convertir
                'audio/webm'
            ];
            
            let selectedMimeType = 'audio/webm'; // Fallback √† convertir
            for (const mimeType of supportedMimeTypes) {
                if (MediaRecorder.isTypeSupported(mimeType)) {
                    selectedMimeType = mimeType;
                    break;
                }
            }

            const mediaRecorder = new MediaRecorder(stream, {
                mimeType: selectedMimeType,
                audioBitsPerSecond: 64000 // Augment√© pour meilleure qualit√© (recommandation Mistral)
            });

            console.log('üé§ Format audio s√©lectionn√©:', selectedMimeType);

            mediaRecorderRef.current = mediaRecorder;
            audioChunksRef.current = [];
            transcriptionBufferRef.current = '';
            pendingTranscriptionsRef.current.clear();

            let segmentCounter = 0;
            // Optimisation selon Mistral : segments de 5-10 secondes pour meilleure pr√©cision
            const SEGMENT_DURATION = 8000; // 8 secondes (dans la plage optimale 5-30s)
            const MIN_AUDIO_SIZE = 8192; // Taille minimale augment√©e pour √©viter segments trop courts
            const MAX_CONCURRENT_TRANSCRIPTIONS = 2; // Limit√© selon les bonnes pratiques

            mediaRecorder.ondataavailable = async (event) => {
                if (event.data.size > MIN_AUDIO_SIZE) {
                    audioChunksRef.current.push(event.data);
                    
                    // Respect des limites de concurrence recommand√©es
                    if (pendingTranscriptionsRef.current.size < MAX_CONCURRENT_TRANSCRIPTIONS) {
                        const audioBlob = new Blob([event.data], { type: selectedMimeType });
                        
                        // V√©rification de la taille selon les limites Mistral (25MB max)
                        if (audioBlob.size > 25 * 1024 * 1024) {
                            console.warn(`‚ö†Ô∏è Segment trop volumineux: ${audioBlob.size} bytes (max 25MB)`);
                            return;
                        }
                        
                        const currentSegmentId = segmentCounter++;
                        pendingTranscriptionsRef.current.add(currentSegmentId);
                        
                        // Transcription asynchrone optimis√©e
                        transcribeSegmentWithVoxtralOptimized(audioBlob, currentSegmentId, selectedMimeType)
                            .finally(() => {
                                pendingTranscriptionsRef.current.delete(currentSegmentId);
                            });
                    } else {
                        console.log('üîÑ Transcriptions en cours, segment ignor√© (√©vite surcharge API)');
                    }
                }
            };

            mediaRecorder.onstop = async () => {
                console.log('üé§ Enregistrement Voxtral termin√©');
                setIsRecording(false);
                setIsTranscribing(false);
                
                // Attendre que toutes les transcriptions en cours se terminent
                const waitForPendingTranscriptions = () => {
                    return new Promise<void>((resolve) => {
                        const checkInterval = setInterval(() => {
                            if (pendingTranscriptionsRef.current.size === 0) {
                                clearInterval(checkInterval);
                                resolve();
                            }
                        }, 100);
                        
                        // Timeout apr√®s 5 secondes
                        setTimeout(() => {
                            clearInterval(checkInterval);
                            resolve();
                        }, 5000);
                    });
                };
                
                await waitForPendingTranscriptions();
                
                // Nettoyer le texte final et appliquer le buffer
                const finalText = transcriptionBufferRef.current.trim();
                if (finalText) {
                    setInputMessage(prev => {
                        const cleanedPrev = prev.replace(/\[Transcription en cours\.\.\.\]/g, '').trim();
                        return cleanedPrev ? `${cleanedPrev} ${finalText}` : finalText;
                    });
                }
                transcriptionBufferRef.current = '';
                
                // Arr√™ter le stream
                stream.getTracks().forEach(track => track.stop());
            };

            mediaRecorder.onerror = (event) => {
                console.error('‚ùå Erreur MediaRecorder:', event);
                setIsRecording(false);
                setIsTranscribing(false);
                pendingTranscriptionsRef.current.clear();
                // Arr√™ter le stream en cas d'erreur
                stream.getTracks().forEach(track => track.stop());
            };

            // D√©marrer l'enregistrement
            mediaRecorder.start();
            setIsRecording(true);
            setIsTranscribing(true);
            
            // Cr√©er des segments optimis√©s selon les recommandations Mistral
            const segmentInterval = setInterval(() => {
                if (mediaRecorder.state === 'recording') {
                    mediaRecorder.requestData();
                }
            }, SEGMENT_DURATION);

            // Stocker l'interval pour le nettoyer
            mediaRecorderRef.current.segmentInterval = segmentInterval;

            console.log('üé§ Enregistrement Voxtral optimis√© d√©marr√©:', {
                segmentDuration: SEGMENT_DURATION + 'ms (recommandation Mistral: 5-30s)',
                minAudioSize: MIN_AUDIO_SIZE + ' bytes',
                maxConcurrent: MAX_CONCURRENT_TRANSCRIPTIONS,
                sampleRate: '16kHz',
                bitrate: '64kbps'
            });
            
        } catch (error) {
            console.error('Erreur lors du d√©marrage de l\'enregistrement Voxtral:', error);
            if (error.name === 'NotAllowedError') {
                alert('Permission microphone refus√©e. Veuillez autoriser l\'acc√®s dans les param√®tres de votre navigateur.');
            } else {
                alert('Erreur microphone. V√©rifiez votre √©quipement audio.');
            }
            setIsRecording(false);
            setIsTranscribing(false);
        }
    };

    // Fonction corrig√©e pour convertir WebM vers WAV compatible avec Mistral AI
    const convertWebMToWav = async (webmBlob: Blob): Promise<Blob> => {
        try {
            console.log('üîÑ D√©but conversion WebM vers WAV conforme Mistral...');
            
            // Configuration AudioContext optimale selon Mistral AI
            const audioContext = new (window.AudioContext || window.webkitAudioContext)({
                sampleRate: 16000, // 16kHz recommand√© par Mistral
                latencyHint: 'playback'
            });
            
            const arrayBuffer = await webmBlob.arrayBuffer();
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
            
            console.log('üìä AudioBuffer analys√©:', {
                sampleRate: audioBuffer.sampleRate,
                numberOfChannels: audioBuffer.numberOfChannels,
                length: audioBuffer.length,
                duration: audioBuffer.duration + 's'
            });
            
            // Param√®tres WAV stricts selon sp√©cifications Mistral
            const targetSampleRate = 16000;
            const numberOfChannels = 1; // Mono obligatoire
            const bitsPerSample = 16; // PCM 16-bit
            const bytesPerSample = bitsPerSample / 8;
            const blockAlign = numberOfChannels * bytesPerSample;
            const byteRate = targetSampleRate * blockAlign;
            
            // Resampling et conversion en mono
            const inputChannelData = audioBuffer.getChannelData(0);
            const inputSampleRate = audioBuffer.sampleRate;
            const resampleRatio = targetSampleRate / inputSampleRate;
            const outputLength = Math.floor(audioBuffer.length * resampleRatio);
            const outputSamples = new Float32Array(outputLength);
            
            // Resampling avec interpolation lin√©aire am√©lior√©e
            for (let i = 0; i < outputLength; i++) {
                const sourceIndex = i / resampleRatio;
                const index = Math.floor(sourceIndex);
                const fraction = sourceIndex - index;
                
                if (index + 1 < inputChannelData.length) {
                    outputSamples[i] = inputChannelData[index] * (1 - fraction) + 
                                      inputChannelData[index + 1] * fraction;
                } else {
                    outputSamples[i] = inputChannelData[Math.min(index, inputChannelData.length - 1)] || 0;
                }
            }
            
            // Calcul des tailles pour le format WAV
            const dataSize = outputLength * bytesPerSample;
            const fileSize = 36 + dataSize;
            
            // Cr√©ation du buffer WAV avec header complet et correct
            const buffer = new ArrayBuffer(44 + dataSize);
            const view = new DataView(buffer);
            
            // Fonction utilitaire pour √©crire des cha√Ænes ASCII
            const writeString = (offset: number, string: string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };
            
            // Header RIFF/WAV complet selon standard WAV
            writeString(0, 'RIFF');                           // ChunkID (4 bytes)
            view.setUint32(4, fileSize, true);                // ChunkSize (4 bytes, little-endian)
            writeString(8, 'WAVE');                           // Format (4 bytes)
            
            // Subchunk1 (fmt)
            writeString(12, 'fmt ');                          // Subchunk1ID (4 bytes)
            view.setUint32(16, 16, true);                     // Subchunk1Size (4 bytes)
            view.setUint16(20, 1, true);                      // AudioFormat (2 bytes) - PCM = 1
            view.setUint16(22, numberOfChannels, true);       // NumChannels (2 bytes)
            view.setUint32(24, targetSampleRate, true);       // SampleRate (4 bytes)
            view.setUint32(28, byteRate, true);               // ByteRate (4 bytes)
            view.setUint16(32, blockAlign, true);             // BlockAlign (2 bytes)
            view.setUint16(34, bitsPerSample, true);          // BitsPerSample (2 bytes)
            
            // Subchunk2 (data)
            writeString(36, 'data');                          // Subchunk2ID (4 bytes)
            view.setUint32(40, dataSize, true);               // Subchunk2Size (4 bytes)
            
            // Conversion des √©chantillons en PCM 16-bit
            let offset = 44;
            for (let i = 0; i < outputLength; i++) {
                // Clamp et conversion en entier 16-bit sign√©
                let sample = Math.max(-1, Math.min(1, outputSamples[i]));
                const intSample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
                view.setInt16(offset, Math.round(intSample), true); // Little-endian
                offset += 2;
            }
            
            // Validation du fichier WAV g√©n√©r√©
            const wavBlob = new Blob([buffer], { 
                type: 'audio/wav'
            });
            
            console.log('‚úÖ Conversion WAV r√©ussie:', {
                inputFormat: 'WebM',
                outputFormat: 'WAV PCM 16-bit',
                inputSize: webmBlob.size + ' bytes',
                outputSize: wavBlob.size + ' bytes',
                sampleRate: targetSampleRate + 'Hz',
                channels: numberOfChannels,
                duration: (outputLength / targetSampleRate).toFixed(2) + 's',
                compression: 'Aucune (PCM)'
            });
            
            return wavBlob;
            
        } catch (error) {
            console.error('‚ùå Erreur conversion WebM vers WAV:', error);
            // Ne plus utiliser de fallback, lancer l'erreur pour diagnostic
            throw new Error(`Conversion impossible: ${error.message}`);
        }
    };

    // Fonction optimis√©e pour la transcription avec conversion WAV
    const transcribeSegmentWithVoxtralOptimized = async (audioBlob: Blob, segmentId: number, originalMimeType: string, retryCount = 0) => {
        const MAX_RETRIES = 2;
        
        try {
            console.log(`üìù Transcription segment ${segmentId} (${audioBlob.size} bytes, ${originalMimeType}) - Tentative ${retryCount + 1}`);
            
            let processedBlob = audioBlob;
            let fileName = `segment_${segmentId}.wav`;
            
            // Convertir WebM vers WAV si n√©cessaire
            if (originalMimeType.includes('webm')) {
                console.log(`üîÑ Conversion WebM vers WAV pour segment ${segmentId}...`);
                processedBlob = await convertWebMToWav(audioBlob);
                console.log(`‚úÖ Conversion termin√©e, nouvelle taille: ${processedBlob.size} bytes`);
            } else if (originalMimeType.includes('wav')) {
                fileName = `segment_${segmentId}.wav`;
            }

            const formData = new FormData();
            formData.append('audio', processedBlob, fileName);

            const controller = new AbortController();
            const timeoutId = setTimeout(() => controller.abort(), 15000); // Timeout 15s pour la conversion

            const response = await fetch('/api/voxtral', {
                method: 'POST',
                body: formData,
                signal: controller.signal
            });

            clearTimeout(timeoutId);

            if (response.ok) {
                const result = await response.json();
                console.log(`‚úÖ Segment ${segmentId} transcrit:`, result.text?.substring(0, 50) + '...');
                
                if (result.text && result.text.trim()) {
                    const cleanText = result.text.trim();
                    
                    // Buffer intelligent pour √©viter les doublons et organiser le texte
                    transcriptionBufferRef.current = (transcriptionBufferRef.current + ' ' + cleanText).trim();
                    
                    // Mise √† jour de l'affichage avec buffer
                    setInputMessage(prev => {
                        const baseText = prev.replace(/\[Transcription en cours...\]/g, '').trim();
                        // Utiliser le buffer pour un texte plus coh√©rent
                        return (baseText ? baseText + ' ' : '') + transcriptionBufferRef.current + ' [Transcription en cours...]';
                    });
                }
            } else if (response.status === 429 && retryCount < MAX_RETRIES) {
                // Rate limiting : attendre et r√©essayer
                console.warn(`‚ö†Ô∏è Rate limit segment ${segmentId}, retry dans 1s...`);
                await new Promise(resolve => setTimeout(resolve, 1000 * (retryCount + 1)));
                return transcribeSegmentWithVoxtralOptimized(audioBlob, segmentId, originalMimeType, retryCount + 1);
            } else {
                console.warn(`‚ö†Ô∏è Erreur transcription segment ${segmentId}:`, response.status, response.statusText);
                const errorText = await response.text();
                console.warn('D√©tails erreur:', errorText);
            }
        } catch (error) {
            if (error.name === 'AbortError') {
                console.warn(`‚è∞ Timeout segment ${segmentId}`);
            } else if (retryCount < MAX_RETRIES) {
                console.warn(`üîÑ Retry segment ${segmentId} (${error.message})`);
                await new Promise(resolve => setTimeout(resolve, 500));
                return transcribeSegmentWithVoxtralOptimized(audioBlob, segmentId, originalMimeType, retryCount + 1);
            } else {
                console.error(`‚ùå √âchec d√©finitif segment ${segmentId}:`, error.message);
            }
        }
    };

    // Fonction pour arr√™ter l'enregistrement Voxtral de mani√®re propre
    const stopVoxtralLiveTranscription = () => {
        if (mediaRecorderRef.current && isRecording) {
            // Nettoyer l'interval des segments
            if (mediaRecorderRef.current.segmentInterval) {
                clearInterval(mediaRecorderRef.current.segmentInterval);
                delete mediaRecorderRef.current.segmentInterval;
            }
            
            // Arr√™ter l'enregistrement (d√©clenchera automatiquement onstop)
            mediaRecorderRef.current.stop();
            console.log('üõë Arr√™t demand√© pour l\'enregistrement Voxtral optimis√©');
        }
    };

    // Syst√®me hybride : Web Speech API pour le temps r√©el + Voxtral pour la pr√©cision
    const startHybridLiveTranscription = async () => {
        // V√©rifier la disponibilit√© de Web Speech API
        const hasWebSpeech = ('webkitSpeechRecognition' in window) || ('SpeechRecognition' in window);
        
        if (hasWebSpeech) {
            // D√©marrer la transcription en temps r√©el avec Web Speech API
            startWebSpeechTranscription();
        }
        
        // En parall√®le, d√©marrer l'enregistrement Voxtral pour validation
        await startVoxtralBackgroundRecording();
    };

    // Transcription instantan√©e avec Web Speech API
    const startWebSpeechTranscription = () => {
        try {
            // @ts-ignore
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            const recognition = new SpeechRecognition();

            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'fr-FR';

            let finalTranscript = '';

            recognition.onstart = () => {
                console.log('üé§ Transcription temps r√©el d√©marr√©e (Web Speech)');
                setIsLiveTranscribing(true);
            };

            recognition.onresult = (event) => {
                let interimTranscript = '';
                
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const transcript = event.results[i][0].transcript;
                    
                    if (event.results[i].isFinal) {
                        finalTranscript += transcript + ' ';
                    } else {
                        interimTranscript = transcript;
                    }
                }

                // Mise √† jour en temps r√©el de l'input
                const baseText = inputMessage.replace(/\[.*?\].*$/, '').trim();
                let displayText = baseText + (baseText ? ' ' : '') + finalTranscript;
                
                if (interimTranscript) {
                    displayText += `[En cours...] ${interimTranscript}`;
                }
                
                setInputMessage(displayText);
            };

            recognition.onend = () => {
                console.log('üé§ Transcription temps r√©el termin√©e');
                setIsLiveTranscribing(false);
                
                // Nettoyer le texte final
                const cleanedText = inputMessage.replace(/\[.*?\].*$/, '').trim();
                setInputMessage(cleanedText);
            };

            recognition.onerror = (event) => {
                console.error('‚ùå Erreur Web Speech:', event.error);
                setIsLiveTranscribing(false);
                
                if (event.error === 'not-allowed') {
                    console.warn('Permission refus√©e pour Web Speech, utilisation Voxtral uniquement');
                }
            };

            recognition.start();
            recognitionRef.current = recognition;
            
        } catch (error) {
            console.error('Web Speech non disponible, utilisation Voxtral uniquement:', error);
        }
    };

    // Enregistrement Voxtral en arri√®re-plan pour validation
    const startVoxtralBackgroundRecording = async () => {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    channelCount: 1,
                    sampleRate: 16000,
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                }
            });
            
            const supportedMimeTypes = [
                'audio/wav',
                'audio/mp4',
                'audio/webm;codecs=opus',
                'audio/webm'
            ];
            
            let selectedMimeType = 'audio/webm';
            for (const mimeType of supportedMimeTypes) {
                if (MediaRecorder.isTypeSupported(mimeType)) {
                    selectedMimeType = mimeType;
                    break;
                }
            }

            const mediaRecorder = new MediaRecorder(stream, {
                mimeType: selectedMimeType,
                audioBitsPerSecond: 64000
            });

            mediaRecorderRef.current = mediaRecorder;
            audioChunksRef.current = [];

            // Enregistrement complet pour validation finale avec Voxtral
            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    audioChunksRef.current.push(event.data);
                }
            };

            mediaRecorder.onstop = async () => {
                console.log('üé§ Enregistrement Voxtral termin√©, validation finale...');
                
                // Cr√©er le fichier audio complet
                const completeAudioBlob = new Blob(audioChunksRef.current, { type: selectedMimeType });
                
                if (completeAudioBlob.size > 8192) { // V√©rifier qu'il y a assez d'audio
                    try {
                        // Valider avec Voxtral
                        await validateWithVoxtral(completeAudioBlob, selectedMimeType);
                    } catch (error) {
                        console.warn('Validation Voxtral √©chou√©e, conservant Web Speech:', error);
                    }
                }
                
                setIsRecording(false);
                setIsTranscribing(false);
                
                // Arr√™ter le stream
                stream.getTracks().forEach(track => track.stop());
            };

            mediaRecorder.onerror = (event) => {
                console.error('‚ùå Erreur MediaRecorder:', event);
                setIsRecording(false);
                setIsTranscribing(false);
                stream.getTracks().forEach(track => track.stop());
            };

            mediaRecorder.start();
            setIsRecording(true);
            setIsTranscribing(true);

            console.log('üìπ Enregistrement Voxtral de validation d√©marr√©');
            
        } catch (error) {
            console.error('Erreur enregistrement Voxtral:', error);
            if (error.name === 'NotAllowedError') {
                alert('Permission microphone refus√©e. Veuillez autoriser l\'acc√®s dans les param√®tres de votre navigateur.');
            }
            setIsRecording(false);
            setIsTranscribing(false);
        }
    };

    // Validation finale avec Voxtral
    const validateWithVoxtral = async (audioBlob: Blob, mimeType: string) => {
        try {
            console.log('üîç Validation finale avec Voxtral...');
            
            let processedBlob = audioBlob;
            
            if (mimeType.includes('webm')) {
                processedBlob = await convertWebMToWav(audioBlob);
            }

            const formData = new FormData();
            formData.append('audio', processedBlob, 'validation.wav');

            const response = await fetch('/api/voxtral', {
                method: 'POST',
                body: formData
            });

            if (response.ok) {
                const result = await response.json();
                
                if (result.text && result.text.trim()) {
                    console.log('‚úÖ Validation Voxtral:', result.text);
                    
                    // Remplacer le texte par la version Voxtral si tr√®s diff√©rente
                    const currentText = inputMessage.replace(/\[.*?\].*$/, '').trim();
                    const voxtralText = result.text.trim();
                    
                    // Si Voxtral donne un r√©sultat significativement diff√©rent, proposer de l'utiliser
                    if (voxtralText.length > currentText.length * 1.2 || voxtralText.length < currentText.length * 0.8) {
                        console.log('üìù Diff√©rence significative d√©tect√©e, suggestion Voxtral');
                        // Optionnel : on peut ajouter une logique pour proposer le texte Voxtral
                    }
                }
            }
        } catch (error) {
            console.warn('Validation Voxtral √©chou√©e:', error);
        }
    };

    // Arr√™ter la transcription hybride
    const stopHybridLiveTranscription = () => {
        // Arr√™ter Web Speech API
        if (recognitionRef.current) {
            recognitionRef.current.stop();
            recognitionRef.current = null;
        }
        
        // Arr√™ter l'enregistrement Voxtral
        if (mediaRecorderRef.current && isRecording) {
            mediaRecorderRef.current.stop();
        }
        
        setIsLiveTranscribing(false);
    };

    // Fonction pour g√©rer le clic sur le bouton microphone
    const handleMicClick = () => {
        if (isRecording) {
            stopHybridLiveTranscription();
        } else {
            startHybridLiveTranscription();
        }
    };

    // Fonction pour g√©rer le TTS natif du navigateur
    const handleNativeTTS = (messageContent: string, messageId: string) => {
        try {
            // V√©rifier si le navigateur supporte la synth√®se vocale
            if (!('speechSynthesis' in window)) {
                alert('Votre navigateur ne supporte pas la synth√®se vocale');
                return;
            }

            // Arr√™ter toute synth√®se en cours
            speechSynthesis.cancel();

            // Cr√©er un √©l√©ment temporaire pour parser le HTML
            const tempDiv = document.createElement('div');
            tempDiv.innerHTML = messageContent;
            
            // R√©cup√©rer sp√©cifiquement le contenu des √©l√©ments avec la classe "leading-relaxed"
            const leadingRelaxedElements = tempDiv.querySelectorAll('.leading-relaxed');
            
            let textContent = '';
            if (leadingRelaxedElements.length > 0) {
                // Extraire le texte de tous les √©l√©ments leading-relaxed
                leadingRelaxedElements.forEach(element => {
                    textContent += element.textContent || '';
                });
            } else {
                // Fallback : utiliser tout le contenu si pas d'√©l√©ment leading-relaxed trouv√©
                textContent = tempDiv.textContent || '';
            }
            
            // Nettoyer le texte
            textContent = textContent
                .replace(/\s+/g, ' ') // Remplacer les espaces multiples par un seul
                .trim();

            if (!textContent || textContent.length === 0) {
                console.error('Texte vide pour TTS natif');
                return;
            }

            console.log('üü† TTS natif pour message:', messageId);
            console.log('Texte √† synth√©tiser:', textContent.substring(0, 100) + (textContent.length > 100 ? "..." : ""));

            // Cr√©er l'utterance pour la synth√®se vocale
            const utterance = new SpeechSynthesisUtterance(textContent);
            
            // Configuration de la voix (essayer de trouver une voix fran√ßaise)
            const voices = speechSynthesis.getVoices();
            const frenchVoice = voices.find(voice => 
                voice.lang.startsWith('fr') || 
                voice.name.toLowerCase().includes('french') ||
                voice.name.toLowerCase().includes('fran√ßais')
            );
            
            if (frenchVoice) {
                utterance.voice = frenchVoice;
                console.log('üé§ Voix fran√ßaise trouv√©e:', frenchVoice.name);
            } else {
                console.log('üé§ Aucune voix fran√ßaise trouv√©e, utilisation de la voix par d√©faut');
            }

            // Configuration des param√®tres
            utterance.rate = 0.9; // Vitesse l√©g√®rement ralentie
            utterance.pitch = 1.0; // Ton normal
            utterance.volume = 1.0; // Volume maximal

            // Gestionnaires d'√©v√©nements
            utterance.onstart = () => {
                console.log('üü† Synth√®se vocale native d√©marr√©e');
                setIsSpeaking(messageId);
            };

            utterance.onend = () => {
                console.log('üü† Synth√®se vocale native termin√©e');
                setIsSpeaking(null);
            };

            utterance.onerror = (event) => {
                console.error('‚ùå Erreur synth√®se vocale native:', event.error);
                setIsSpeaking(null);
            };

            // D√©marrer la synth√®se
            speechSynthesis.speak(utterance);

        } catch (error) {
            console.error('‚ùå Erreur TTS natif:', error);
            setIsSpeaking(null);
        }
    };

    const handleSubmit = async (e: React.FormEvent) => {
        e.preventDefault();
        if (!inputMessage.trim()) return;

        const userMessage: Message = {
            id: Date.now().toString(),
            type: 'user',
            content: inputMessage,
            timestamp: new Date()
        };

        setMessages(prev => [...prev, userMessage]);
        const currentMessage = inputMessage;
        setInputMessage('');

        try {
            const response = await fetch('/api/chat', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    message: currentMessage,
                    sessionId: sessionId,
                    timestamp: new Date().toISOString()
                })
            });
        
            if (response.ok) {
                const data = await response.json();
                let parsedContent = await marked.parse(data.response) || 'R√©ponse re√ßue du webhook';
                
                // Transformer les balises <em> en balises avec style personnalis√© pour les messages du bot
                parsedContent = parsedContent.replace(/<em>(.*?)<\/em>/g, '<span style="font-weight: bold; color: #F28C06;">$1</span>');
                
                // Diviser le message si il contient la balise <hr>
                const messageParts = parsedContent.split('<hr>');
                
                if (messageParts.length > 1) {
                    // Plusieurs messages √† cr√©er
                    messageParts.forEach((part, index) => {
                        if (part.trim()) { // Ne pas cr√©er de message vide
                            const aiMessage: Message = {
                                id: (Date.now() + index + 1).toString(),
                                type: 'ai',
                                content: part.trim(),
                                timestamp: new Date()
                            };
                            setMessages(prev => [...prev, aiMessage]);
                        }
                    });
                } else {
                    // Message unique
                    const aiMessage: Message = {
                        id: (Date.now() + 1).toString(),
                        type: 'ai',
                        content: parsedContent,
                        timestamp: new Date()
                    };
                    setMessages(prev => [...prev, aiMessage]);
                }
            } else {
                throw new Error('Erreur lors de l\'appel au webhook');
            }
        } catch (error) {
            const errorMessage: Message = {
                id: (Date.now() + 1).toString(),
                type: 'ai',
                content: 'D√©sol√©, une erreur est survenue lors du traitement de votre demande.',
                timestamp: new Date()
            };
            setMessages(prev => [...prev, errorMessage]);
        }
    };

    // Afficher un loader pendant la v√©rification
    if (status === 'loading') {
        return (
            <div className="relative min-h-screen flex justify-center items-center">
                <div className="absolute inset-0 z-0">
                    <Particles
                        particleColors={['#43bb8c']}
                        particleCount={300}
                        particleSpread={6}
                        speed={0.05}
                        particleBaseSize={80}
                        moveParticlesOnHover={false}
                        alphaParticles={true}
                        disableRotation={false}
                    />
                </div>
                <div className="relative z-10 text-lg text-gray-600">
                    V√©rification de votre session...
                </div>
            </div>
        )
    }

    // Si pas de session, ne rien afficher (la redirection va se faire)
    if (!session) {
        return null
    }

    // Fonction de d√©connexion
    const handleSignOut = async () => {
        await signOut({ callbackUrl: '/Login' })
    }

    return (
        <div className="relative min-h-screen flex flex-col">
            {/* Particles Background */}
            <div className="absolute inset-0 z-0">
                <Particles
                    particleColors={['#43bb8c']}
                    particleCount={300}
                    particleSpread={6}
                    speed={0.05}
                    particleBaseSize={80}
                    moveParticlesOnHover={false}
                    alphaParticles={true}
                    disableRotation={false}
                />
            </div>

            {/* Main Chat Container */}
            <div className="relative z-10 flex-1 flex flex-col max-w-4xl mx-auto w-full p-4">
                
                {/* Header Card */}
                <Card className="mb-2 bg-white/95 shadow-lg border border-gray-200">
                    <CardHeader className="pb-3 bg-transparent border-gray-200">
                        <div className="flex justify-between items-center">
                            <div>
                                <CardTitle className="text-xl sm:text-2xl font-bold text-gray-800">
                                    E2I AgentSecu
                                </CardTitle>
                                <CardDescription className="text-gray-600 max-sm:hidden">
                                    Assistant IA pour vos questions de s√©curit√©
                                </CardDescription>
                            </div>
                            <div className="flex items-center gap-2">

                                <Button
                                    onClick={handleSignOut}
                                    className="text-xs"
                                >
                                    <SessionId />
                                </Button>
                            </div>
                        </div>
                        <CardContent className="text-gray-600 text-sm sm:hidden block p-0">
                            Assistant IA - {session.user?.email}
                        </CardContent>
                    </CardHeader>
                </Card>

                {/* Messages Area */}
                <Card className="flex-1 mb-2 bg-white/95 shadow-lg border border-gray-200 flex flex-col">
                    <CardContent className="flex-1 p-6 overflow-y-auto max-h-[60vh]">
                        <div className="space-y-4">
                            {messages.map((message) => (
                                <div
                                    key={message.id}
                                    className={`flex ${message.type === 'user' ? 'justify-end' : 'justify-start'}`}
                                >
                                    <div className="relative group max-w-[80%]">
                                        <div
                                            className={`p-4 rounded-2xl ${
                                                message.type === 'user'
                                                    ? 'bg-[#43bb8c] text-white rounded-br-md'
                                                    : 'bg-gray-100 text-gray-800 rounded-bl-md'
                                            }`}
                                        >
                                            <div 
                                                className="text-sm leading-relaxed"
                                                dangerouslySetInnerHTML={{ __html: message.content }}
                                            />
                                            <span className={`text-xs mt-2 block ${
                                                message.type === 'user' ? 'text-green-100' : 'text-gray-500'
                                            }`}>
                                                {message.timestamp.toLocaleTimeString('fr-FR', { 
                                                    hour: '2-digit', 
                                                    minute: '2-digit' 
                                                })}
                                            </span>
                                        </div>
                                        
                                        {/* Bouton TTS Natif du navigateur */}
                                        <div className={`absolute -top-2 ${message.type === 'user' ? '-left-12' : '-right-12'} 
                                            opacity-0 group-hover:opacity-100 transition-opacity duration-200`}>
                                            <button
                                                onClick={() => handleNativeTTS(message.content, message.id)}
                                                disabled={isSpeaking === message.id}
                                                className="bg-white shadow-lg rounded-full p-2 border border-gray-200
                                                    hover:bg-orange-50 disabled:opacity-50 disabled:cursor-not-allowed"
                                                title="√âcouter avec la voix du navigateur (gratuit et rapide)"
                                            >
                                                {isSpeaking === message.id ? (
                                                    <div className="w-4 h-4 animate-pulse">
                                                        <span className="text-orange-600 text-sm">üü†</span>
                                                    </div>
                                                ) : (
                                                    <span className="text-orange-600 text-sm">üü†</span>
                                                )}
                                            </button>
                                        </div>
                                    </div>
                                </div>
                            ))}
                            <div ref={messagesEndRef} />
                        </div>
                    </CardContent>
                </Card>

                {/* Input Area */}
                <Card className="bg-transparent shadow-none border-0">
                    <CardFooter className="p-0 bg-transparent border-0">
                        <form onSubmit={handleSubmit} className="w-full">
                            <div className="flex gap-2 items-end bg-gray-50 rounded-3xl p-2 border-2 border-gray-200 focus-within:border-[#43bb8c] transition-colors">
                                <textarea
                                    ref={textareaRef}
                                    value={inputMessage}
                                    onChange={(e) => setInputMessage(e.target.value)}
                                    placeholder="Tapez votre message ici..."
                                    className="flex-1 min-h-[44px] bg-transparent border-none focus:ring-0 focus:outline-none text-gray-800 resize-none"
                                />
                                <Button
                                    type="submit"
                                    disabled={!inputMessage.trim()}
                                    className="bg-[#43bb8c] hover:bg-[#3aa078] disabled:bg-gray-400 text-white px-4 py-2 h-[44px] rounded-full transition-colors"
                                >
                                    ‚¨ÜÔ∏è
                                </Button>
                                <Button
                                    type="button"
                                    variant="outline"
                                    className="border-[#43bb8c] text-[#43bb8c] hover:bg-[#43bb8c] hover:text-white transition-colors px-4 py-2 h-[44px] rounded-full"
                                    onClick={handleMicClick}
                                    disabled={isTranscribing}
                                >
                                    {isRecording ? '‚èπÔ∏è' : 'üé§'}
                                </Button>
                            </div>
                        </form>

                    </CardFooter>
                </Card>
            </div>
        </div>
    );
}

